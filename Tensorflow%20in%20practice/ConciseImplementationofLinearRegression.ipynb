{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ConciseImplementationofTensorflow.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNww8CQWu5DRnkQU0ZYsWlk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TeAmP0is0N/Repo-2020/blob/master/Tensorflow%2520in%2520practice/ConciseImplementationofLinearRegression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p75KC0VXKZ3l",
        "colab_type": "text"
      },
      "source": [
        "Implementation of Linear Regression by using high level APIs of Deep Learning frameworks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmqcfA4xLeQ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install -U d2l"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyHFnShEJ8yU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from d2l import tensorflow as d2l\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D53SXIB6KFNt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "true_w = tf.constant([2, -3.4])\n",
        "true_b = 4.2\n",
        "features, labels = d2l.synthetic_data(true_w, true_b, 1000)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7qOJFQwLyy7",
        "colab_type": "text"
      },
      "source": [
        "### Reading the Dataset\n",
        "Rather than rolling our own iterator, we can call upon the existing API in a framework to read data. We pass in `features` and `labels` as arguments and specify `batch_size` when instantiating a data iterator object. Besides, the boolean value is_train indicates whether or not we want the data iterator object to shuffle the data on each epoch (pass through the dataset)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9OfkSBULBQV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_array(data_arrays, batch_size, is_train=True):\n",
        "  \"\"\"Construct a Tensorflow data iterator.\"\"\"\n",
        "  dataset = tf.data.Dataset.from_tensor_slices(data_arrays)\n",
        "  if is_train:\n",
        "    dataset = dataset.shuffle(buffer_size=100)\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  return dataset\n",
        "\n",
        "batch_size = 10\n",
        "data_iter = load_array((features, labels), batch_size)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6TU_rXcNbTv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "5b5e514f-129b-42e2-a825-c3369ac224b2"
      },
      "source": [
        "next(iter(data_iter))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(10, 2), dtype=float32, numpy=\n",
              " array([[-0.48509452, -1.0031143 ],\n",
              "        [-1.2455248 , -1.0678812 ],\n",
              "        [-0.4060649 , -1.3355876 ],\n",
              "        [ 0.61740464, -0.78729147],\n",
              "        [ 0.00265126,  0.49059355],\n",
              "        [ 2.4839485 , -0.2647907 ],\n",
              "        [-0.19049111, -0.7339733 ],\n",
              "        [-1.0771749 , -0.33752418],\n",
              "        [-0.38383046,  1.2489277 ],\n",
              "        [ 0.7332773 , -1.1825895 ]], dtype=float32)>,\n",
              " <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
              " array([[ 6.6360393],\n",
              "        [ 5.3382854],\n",
              "        [ 7.9428115],\n",
              "        [ 8.126984 ],\n",
              "        [ 2.5201936],\n",
              "        [10.0890665],\n",
              "        [ 6.317945 ],\n",
              "        [ 3.2030745],\n",
              "        [-0.8054451],\n",
              "        [ 9.695987 ]], dtype=float32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiKkZzwIN658",
        "colab_type": "text"
      },
      "source": [
        "### Defining the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LtBY8kKN1qI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# `keras` is the high level API for Tensorflow\n",
        "net = tf.keras.Sequential()\n",
        "net.add(tf.keras.layers.Dense(1))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-MJF5ydOYAE",
        "colab_type": "text"
      },
      "source": [
        "### Initializing Model Parameters\n",
        "Before using net, we need to initialize the model parameters, such as the weights and bias in the linear regression model. Deep learning frameworks often have a predefined way to initialize the parameters. Here we specify that each weight parameter should be randomly sampled from a normal distribution with mean 0 and standard deviation 0.01. The bias parameter will be initialized to zero.\n",
        "The `initializers` module in TensorFlow provides various methods for model parameter initialization. The easiest way to specify the initialization method in Keras is when creating the layer by specifying `kernel_initializer`. Here we recreate net again.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_OoAWx1dOKNZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "initializer = tf.initializers.RandomNormal(stddev=0.01)\n",
        "net = tf.keras.Sequential()\n",
        "net.add(tf.keras.layers.Dense(1, kernel_initializer=initializer))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnFI6_TlPT2l",
        "colab_type": "text"
      },
      "source": [
        "The code above may look straightforward but you should note that something strange is happening here. We are initializing parameters for a network even though Keras does not yet know how many dimensions the input will have! It might be 2 as in our example or it might be 2000. Keras lets us get away with this because behind the scenes, the initialization is actually deferred. The real initialization will take place only when we for the first time attempt to pass data through the network. Just be careful to remember that since the parameters have not been initialized yet, we cannot access or manipulate them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lFVqiArPVZe",
        "colab_type": "text"
      },
      "source": [
        "### Defining the Loss Function\n",
        "The MeanSquaredError class computes the mean squared error, also known as squared  $L_2$  norm. By default it returns the average loss over examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Z33Yrp_PIxG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = tf.keras.losses.MeanSquaredError()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XKmupzKVPsA8",
        "colab_type": "text"
      },
      "source": [
        "### Defining the Optimization Algorithm\n",
        "Minibatch stochastic gradient descent is a standard tool for optimizing neural networks and thus Keras supports it alongside a number of variations on this algorithm in the optimizers module. Minibatch stochastic gradient descent just requires that we set the value `learning_rate`, which is set to 0.03 here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lMYUODY_PnoT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = tf.keras.optimizers.SGD(learning_rate=0.03)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cVRkf31QBQW",
        "colab_type": "text"
      },
      "source": [
        "### Training\n",
        "You might have noticed that expressing our model through high-level APIs of a deep learning framework requires comparatively few lines of code. We did not have to individually allocate parameters, define our loss function, or implement minibatch stochastic gradient descent. Once we start working with much more complex models, advantages of high-level APIs will grow considerably. However, once we have all the basic pieces in place, the training loop itself is strikingly similar to what we did when implementing everything from scratch.\n",
        "To refresh your memory: for some number of epochs, we will make a complete pass over the dataset (train_data), iteratively grabbing one minibatch of inputs and the corresponding ground-truth labels. For each minibatch, we go through the following ritual:\n",
        "\n",
        "- Generate predictions by calling `net(X)` and calculate the `loss l` (the forward propagation).\n",
        "- Calculate gradients by running the backpropagation.\n",
        "- Update the model parameters by invoking our optimizer.\n",
        "For good measure, we compute the loss after each epoch and print it to monitor progress."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvjgo-qoP-ls",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "66f2818f-aaeb-4188-ffcb-6245918f5e79"
      },
      "source": [
        "num_epochs = 3\n",
        "for epoch in range(num_epochs):\n",
        "  for X, y in data_iter:\n",
        "    with tf.GradientTape() as tape:\n",
        "      l = loss(net(X, training=True), y)\n",
        "    grads = tape.gradient(l, net.trainable_variables)\n",
        "    trainer.apply_gradients(zip(grads, net.trainable_variables))\n",
        "  l = loss(net(features), labels)\n",
        "  print(f'epoch {epoch + 1}, loss {1:f}')"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1, loss 1.000000\n",
            "epoch 2, loss 1.000000\n",
            "epoch 3, loss 1.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOjvSBdHRg_t",
        "colab_type": "text"
      },
      "source": [
        "Below, we compare the model parameters learned by training on finite data and the actual parameters that generated our dataset. To access parameters, we first access the layer that we need from net and then access that layer’s weights and bias. As in our from-scratch implementation, note that our estimated parameters are close to their ground-truth counterparts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28EhaPAWRSut",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "763862ef-60ae-47f7-f3e2-7c23c364b7c3"
      },
      "source": [
        "w = net.get_weights()[0]\n",
        "print('error in estimating w', true_w - tf.reshape(w, true_w.shape))\n",
        "b = net.get_weights()[1]\n",
        "print('error in estimating b', true_b - b)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "error in estimating w tf.Tensor([-2.4557114e-04  2.6226044e-05], shape=(2,), dtype=float32)\n",
            "error in estimating b [0.00099897]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7LRg8fiRjJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}